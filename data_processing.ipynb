{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a81e9fa",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29b5300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "from scipy.stats import iqr\n",
    "import shutil\n",
    "import gdown\n",
    "import zipfile\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038ad0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c701546f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gdown.download(url=\"https://drive.google.com/file/d/1PGqqp09sCKfmFE1oewx0m7I2Xdl9eu8e/view?usp=sharing\", fuzzy=True)\n",
    "#with zipfile.ZipFile(\"data_2024-02-17.zip\", \"r\") as archive:\n",
    "#    archive.extractall(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d16b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define useful variables\n",
    "path_data = os.path.join(os.getcwd(), \"data/\")\n",
    "print(f\"path_data: {path_data}\")\n",
    "path_labels = os.path.join(path_data, \"derivatives\", \"labels\")\n",
    "path_qc = os.path.join(path_data, \"qc\")\n",
    "subjects = [os.path.basename(subject_path) for subject_path in sorted(glob.glob(os.path.join(path_data, \"sub-*\")))]\n",
    "print(f\"subjects: {subjects}\")\n",
    "\n",
    "# Create output folder\n",
    "path_results = os.path.join(path_data, \"derivatives\", \"results\")\n",
    "os.makedirs(path_results, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5656d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove SpinozaV6 for now\n",
    "for subject in subjects:\n",
    "    if subject[-1] == \"4\":\n",
    "        subjects.remove(subject)\n",
    "print(subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbc89f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MP2RAGE segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a71c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run segmentation on MP2RAGE scan\n",
    "\n",
    "for subject in subjects:\n",
    "    os.chdir(os.path.join(path_data, subject, \"anat\"))\n",
    "    fname_manual_seg = os.path.join(path_labels, subject, \"anat\", f\"{subject}_UNIT1-SC_seg.nii.gz\")\n",
    "    if os.path.exists(fname_manual_seg):\n",
    "        # Manual segmentation already exists. Copy it to local folder\n",
    "        print(f\"{subject}: Manual segmentation found\\n\")\n",
    "        shutil.copyfile(fname_manual_seg, f\"{subject}_UNIT1_seg.nii.gz\")\n",
    "        # Generate QC report to make sure the manual segmentation is correct\n",
    "        !sct_qc -i {subject}_UNIT1.nii.gz -s {subject}_UNIT1_seg.nii.gz -p sct_deepseg_sc -qc {path_qc} -qc-subject {subject}\n",
    "    else:\n",
    "        # Manual segmentation does not exist. Run automatic segmentation.\n",
    "        print(f\"{subject}: Manual segmentation not found\")\n",
    "        !sct_deepseg_sc -i \"{subject}_UNIT1.nii.gz\" -c t1 -qc \"{path_qc}\"\n",
    "        \n",
    "    # Dilate SC segmentation \n",
    "    !sct_maths -i {subject}_UNIT1_seg.nii.gz -o {subject}_UNIT1_seg_dilated.nii.gz -dilate 15 -dim=2 -shape=disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ce5675",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label vertebrae\n",
    "\n",
    "for subject in subjects:\n",
    "    os.chdir(os.path.join(path_data, subject, \"anat\"))\n",
    "    fname_manual_labels = os.path.join(path_labels, subject, \"anat\", f\"{subject}_UNIT1_label-disks_dseg.nii.gz\")\n",
    "    if os.path.exists(fname_manual_labels):\n",
    "        # Manual labels already exist. Copy file to local folder\n",
    "        print(f\"{subject}: Manual labels found\\n\")\n",
    "        shutil.copyfile(fname_manual_labels, f\"{subject}_UNIT1_seg_labeled_discs.nii.gz\") \n",
    "        !sct_label_utils -i {subject}_UNIT1_seg.nii.gz -disc {subject}_UNIT1_seg_labeled_discs.nii.gz -o {subject}_UNIT1_seg_labeled.nii.gz\n",
    "        # Generate QC report to assess labeled segmentation\n",
    "        !sct_qc -i {subject}_UNIT1_seg.nii.gz -s {subject}_UNIT1_seg_labeled.nii.gz -p sct_label_vertebrae -qc {path_qc} -qc-subject {subject}\n",
    "    else:\n",
    "        # Manual labels do not exist. Run vertebrae labeling.\n",
    "        print(f\"{subject}: Manual labels not found\")\n",
    "        !sct_label_vertebrae -i {subject}_UNIT1.nii.gz -s {subject}_UNIT1_seg.nii.gz -c t1 -qc {path_qc} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f52598",
   "metadata": {},
   "source": [
    "## Process fmap/TFL (flip angle maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847845a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register TFL flip angle maps to the MP2RAGE scan â³\n",
    "\n",
    "for subject in subjects:\n",
    "    os.chdir(os.path.join(path_data, subject, \"fmap\"))\n",
    "    !sct_register_multimodal -i {subject}_acq-anat_TB1TFL.nii.gz -d ../anat/{subject}_UNIT1.nii.gz -dseg ../anat/{subject}_UNIT1_seg.nii.gz -param step=1,type=im,algo=slicereg,metric=CC -qc \"{path_qc}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0d9c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warping spinal cord segmentation and vertebral level to each flip angle map\n",
    "\n",
    "for subject in subjects:\n",
    "    os.chdir(os.path.join(path_data, subject, \"fmap\"))\n",
    "    !sct_apply_transfo -i ../anat/{subject}_UNIT1_seg.nii.gz -d {subject}_acq-anat_TB1TFL.nii.gz -w warp_{subject}_UNIT12{subject}_acq-anat_TB1TFL.nii.gz -x linear -o {subject}_acq-anat_TB1TFL_seg.nii.gz\n",
    "    !sct_apply_transfo -i ../anat/{subject}_UNIT1_seg_labeled.nii.gz -d {subject}_acq-anat_TB1TFL.nii.gz -w warp_{subject}_UNIT12{subject}_acq-anat_TB1TFL.nii.gz -x nn -o {subject}_acq-anat_TB1TFL_seg_labeled.nii.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979a3405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the flip angle maps to B1+ efficiency maps [nT/V] (inspired by code from Kyle Gilbert)\n",
    "# The approach consists in calculating the B1+ efficiency using a 1ms, pi-pulse at the acquisition voltage,\n",
    "# then scale the efficiency by the ratio of the measured flip angle to the requested flip angle in the pulse sequence.\n",
    "\n",
    "GAMMA = 2.675e8;  # [rad / (s T)]\n",
    "requested_fa = 90  # saturation flip angle -- hard-coded in sequence\n",
    "\n",
    "for subject in subjects:\n",
    "    os.chdir(os.path.join(path_data, subject, \"fmap\"))\n",
    "    # Fetch the reference voltage from the JSON sidecar to the TFL B1map sequence\n",
    "    with open(f\"{subject}_acq-famp_TB1TFL.json\", \"r\") as f:\n",
    "        metadata = json.load(f)\n",
    "        ref_voltage = metadata.get(\"TxRefAmp\", \"N/A\")\n",
    "        if (ref_voltage == \"N/A\"):\n",
    "            ref_token = \"N/A\"\n",
    "            for token in metadata.get(\"SeriesDescription\", \"N/A\").split(\"_\"):\n",
    "                if token.startswith(\"RefV\"): ref_token = token\n",
    "            ref_voltage = float(ref_token[4:-1])\n",
    "        print(f\"ref_voltage [V]: {ref_voltage} ({subject}_acq-famp_TB1TFL)\")\n",
    "\n",
    "    # Open flip angle map with nibabel\n",
    "    nii = nib.load(f\"{subject}_acq-famp_TB1TFL.nii.gz\")\n",
    "    acquired_fa = nii.get_fdata()\n",
    "\n",
    "    # Siemens maps are in units of flip angle * 10 (in degrees)\n",
    "    acquired_fa = acquired_fa / 10\n",
    "\n",
    "    # Account for the power loss between the coil and the socket. That number was given by Siemens.\n",
    "    voltage_at_socket = ref_voltage * 10 ** -0.095\n",
    "\n",
    "    # Compute B1 map in [T/V]\n",
    "    b1_map = (acquired_fa / requested_fa) * (np.pi / (GAMMA * 1e-3 * voltage_at_socket))\n",
    "\n",
    "    # Convert to [nT/V]\n",
    "    b1_map = b1_map * 1e9\n",
    "\n",
    "    # Save as NIfTI file\n",
    "    nii_b1 = nib.Nifti1Image(b1_map, nii.affine, nii.header)\n",
    "    nib.save(nii_b1, f\"{subject}_TB1map.nii.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18f5430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract B1+ value along the spinal cord between levels C3 and T2 (included)\n",
    "\n",
    "for subject in subjects:\n",
    "    os.chdir(os.path.join(path_data, subject, \"fmap\"))\n",
    "    fname_result_b1plus = os.path.join(path_results, f\"{subject}_TB1map.csv\")\n",
    "    !sct_extract_metric -i {subject}_TB1map.nii.gz -f {subject}_acq-anat_TB1TFL_seg_dilated.nii.gz -method wa -vert 3:9 -vertfile {subject}_acq-anat_TB1TFL_seg_labeled.nii.gz -perslice 1 -o \"{fname_result_b1plus}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df7c154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal_extractor_from_csv(csv_filename):\n",
    "  # Load the CSV file into a Pandas DataFrame\n",
    "  dataframe = pd.read_csv(csv_filename, index_col='Slice (I->S)')\n",
    "  # Convert all the strings (123.4242, etc) of the WA column into actual numerical values\n",
    "  dataframe['WA()'] = pd.to_numeric(dataframe['WA()'], errors='coerce')\n",
    "  WA_matrix = dataframe['WA()'].to_numpy()\n",
    "  return WA_matrix\n",
    "\n",
    "b1_iqrs = []\n",
    "TFL_B1_nTpV_along_cord_list = []\n",
    "\n",
    "for subject in subjects:\n",
    "    TFL_B1_nTpV_along_cord = signal_extractor_from_csv(os.path.join(path_results, subject + \"_TB1map.csv\"))\n",
    "    TFL_B1_nTpV_along_cord = TFL_B1_nTpV_along_cord[~np.isnan(TFL_B1_nTpV_along_cord)] # remove NaNs\n",
    "    TFL_B1_nTpV_along_cord_list.append(TFL_B1_nTpV_along_cord)\n",
    "    plt.plot(TFL_B1_nTpV_along_cord)\n",
    "    TFL_B1_nTpV_along_cord_mean = np.round(np.mean(TFL_B1_nTpV_along_cord))\n",
    "    std = np.std(TFL_B1_nTpV_along_cord)\n",
    "    titlestring= f\"B1+ efficiency [nT/V] along the cord in the warped mask. Mean value: {TFL_B1_nTpV_along_cord_mean} Std: {std}\"\n",
    "    b1_iqrs.append(iqr(TFL_B1_nTpV_along_cord))\n",
    "    plt.title(titlestring)\n",
    "    plt.xlabel('Slice along the I-->S direction')\n",
    "    plt.ylabel('nT/V')\n",
    "    plt.show()  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e078fbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = [\"MGH\", \"MNI\"]\n",
    "subject_names = [\"Subject D\", \"Subject L\", \"Subject R\"]\n",
    "\n",
    "fig = plt.figure()\n",
    "gs = fig.add_gridspec(1, 2, wspace=0)\n",
    "axs = gs.subplots(sharex=True, sharey=True)\n",
    "fig.set_size_inches(16, 6)\n",
    "\n",
    "j = 0\n",
    "for i in range(len(sites)): \n",
    "    axs[i].plot(TFL_B1_nTpV_along_cord_list[j])\n",
    "    axs[i].plot(TFL_B1_nTpV_along_cord_list[j+1])\n",
    "    axs[i].plot(TFL_B1_nTpV_along_cord_list[j+2])\n",
    "    axs[i].set_title(sites[i])\n",
    "    j = j + 3\n",
    "\n",
    "axs[i].legend(subject_names,loc=\"upper right\")\n",
    "\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel='Slice along the I-->S direction', ylabel='nT/V')\n",
    "\n",
    "# Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9e3634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go back to root data folder\n",
    "os.chdir(os.path.join(path_data))\n",
    "\n",
    "def smooth_data(data, window_size=20):\n",
    "    \"\"\" Apply a simple moving average to smooth the data. \"\"\"\n",
    "    return uniform_filter1d(data, size=window_size, mode='nearest')\n",
    "\n",
    "# Fixed grid for x-axis\n",
    "x_grid = np.linspace(0, 1, 100)\n",
    "\n",
    "# z-slices corresponding to levels C3 to T2 on the PAM50 template. These will be used to scale the x-label of each subject.\n",
    "original_vector = np.array([907, 870, 833, 800, 769, 735, 692, 646])\n",
    "\n",
    "# Normalize the PAM50 z-slice numbers to the 1-0 range (to show inferior-superior instead of superior-inferior)\n",
    "min_val = original_vector.min()\n",
    "max_val = original_vector.max()\n",
    "normalized_vector = 1 - ((original_vector - min_val) / (max_val - min_val))\n",
    "\n",
    "# Use this normalized vector as x-ticks\n",
    "custom_xticks = normalized_vector\n",
    "\n",
    "# Vertebral level labels\n",
    "vertebral_levels = [\"C3\", \"C4\", \"C5\", \"C6\", \"C7\", \"T1\", \"T2\"]\n",
    "# Calculate midpoints for label positions\n",
    "label_positions = normalized_vector[:-1] + np.diff(normalized_vector) / 2\n",
    "\n",
    "# Number of sites determines the number of rows in the subplot\n",
    "sites = [\"MGH\", \"MNI\"]\n",
    "n_rows = len(sites)\n",
    "\n",
    "# Data storage for statistics\n",
    "data_stats = []\n",
    "\n",
    "# Data storage for Plotly\n",
    "b1_data_plotly = {}\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "# Iterate over each site and create a subplot\n",
    "for site in sites:\n",
    "    \n",
    "    b1_data_plotly[site]={}\n",
    "    \n",
    "    while i < (j+3):\n",
    "        \n",
    "        os.chdir(os.path.join(path_data, f\"{subjects[i]}\", \"fmap\"))\n",
    "        \n",
    "        # Initialize list to collect data for this subject\n",
    "        subject_data = []\n",
    "\n",
    "        file_csv = os.path.join(path_results, f\"{subjects[i]}_TB1map.csv\")\n",
    "        df = pd.read_csv(file_csv)\n",
    "        wa_data = df['WA()']\n",
    "\n",
    "        # Normalize the x-axis to a 1-0 scale for each subject (to go from superior-inferior direction)\n",
    "        x_subject = np.linspace(1, 0, len(wa_data))\n",
    "\n",
    "        # Interpolate to the fixed grid\n",
    "        interp_func = interp1d(x_subject, wa_data, kind='linear', bounds_error=False, fill_value='extrapolate')\n",
    "        resampled_data = interp_func(x_grid)\n",
    "\n",
    "        # Apply smoothing\n",
    "        smoothed_data = smooth_data(resampled_data)\n",
    "\n",
    "        subject_data.append(smoothed_data)\n",
    "\n",
    "        # If there's data for this shim method, plot it\n",
    "        if subject_data:\n",
    "            # Plotting each file's data separately\n",
    "            for resampled_data in subject_data:\n",
    "                b1_data_plotly[site][subjects[i]]=resampled_data\n",
    "\n",
    "            # Compute stats on the non-resampled data (to avoid interpolation errors)\n",
    "            mean_data = np.mean(wa_data)\n",
    "            sd_data = np.std(wa_data)\n",
    "            #data_stats.append([site, subjects[i], mean_data, sd_data])\n",
    "            data_stats.append([site, subjects[i], sd_data])\n",
    "        else:\n",
    "            b1_data_plotly[site][subjects[i]]=None\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "    j += 3\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d343fb31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sites = [\"MGH\", \"MNI\"]\n",
    "subject_names = [\"Subject D\", \"Subject L\", \"Subject R\"]\n",
    "\n",
    "fig = plt.figure()\n",
    "gs = fig.add_gridspec(1, 2, wspace=0)\n",
    "axs = gs.subplots(sharex=True, sharey=True)\n",
    "fig.set_size_inches(16, 6)\n",
    "\n",
    "    \n",
    "j = 0\n",
    "i = 0\n",
    "for k, site in enumerate(sites):    \n",
    "    while i < (j+3):\n",
    "        axs[k].plot(b1_data_plotly[site][subjects[i]])\n",
    "        axs[k].set_title(sites[k])\n",
    "        axs[k].grid()\n",
    "        i += 1\n",
    "    j += 3\n",
    "    \n",
    "axs[0].legend(subject_names,loc=\"upper right\")\n",
    "           \n",
    "           \n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel='Vertebral Levels', ylabel='B1+ efficiency [nT/V]', xticks=100*label_positions, xticklabels=vertebral_levels)\n",
    "\n",
    "# Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f07109d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "series = [data_stats[i::len(subject_names)] for i in range(len(subject_names))]\n",
    "hline_x = np.array([0, 1])\n",
    "hline_width = 0.25\n",
    "\n",
    "sub_sd = np.zeros((len(subject_names),len(sites)))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "i = 0 \n",
    "for subject_name, subject_series in zip(subject_names, series):    \n",
    "    ax.scatter(sites, [subject_series[k][2] for k in range(0,len(sites))], label=subject_name)\n",
    "    \n",
    "    for j in range(len(sites)):\n",
    "        sub_sd[i][j] = subject_series[j][2]\n",
    "    \n",
    "    i+=1\n",
    "    \n",
    "plt.hlines(np.mean(sub_sd, axis=0),hline_x - hline_width/2, hline_x + hline_width/2, color=\"black\", label=\"Across subj. mean\")\n",
    "\n",
    "\n",
    "ax.legend()\n",
    "ax.set_ylim(2, 8)\n",
    "ax.set_title(\"SD B1+: standard deviation across slices of SC-averaged signal\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01f7fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = [b1_iqrs[i::3] for i in range(3)]\n",
    "hline_x = np.array([0, 1])\n",
    "hline_width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for subject_name, subject_series in zip(subject_names, series):\n",
    "    ax.scatter(sites, subject_series, label=subject_name)\n",
    "plt.hlines([np.mean(b1_iqrs[i:i+2]) for i in range(0, len(b1_iqrs), 3)], hline_x - hline_width/2, hline_x + hline_width/2, color=\"black\", label=\"Across subj. mean\")\n",
    "ax.legend()\n",
    "ax.set_ylim(2, 20)\n",
    "ax.set_title(\"IQR B1+: IQR across slices within SC ROI\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaba103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicate duration of data processing\n",
    "\n",
    "end_time = datetime.now()\n",
    "total_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Convert seconds to a timedelta object\n",
    "total_time_delta = timedelta(seconds=total_time)\n",
    "\n",
    "# Format the timedelta object to a string\n",
    "formatted_time = str(total_time_delta)\n",
    "\n",
    "# Pad the string representation if less than an hour\n",
    "formatted_time = formatted_time.rjust(8, '0')\n",
    "\n",
    "print(f\"Total Runtime [hour:min:sec]: {formatted_time}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
